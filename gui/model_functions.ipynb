{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from tqdm import tqdm \n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Environment ##\n",
    "def update_n(n, z, e1):\n",
    "    \"\"\"\n",
    "    Update the resource state based on extraction and environmental impact.\n",
    "\n",
    "    Parameters:\n",
    "    - n (float): Current resource state.\n",
    "    - z (float): Extraction level.\n",
    "    - e1 (float): Relative speed of environmental dynamics to strategic dynamics.\n",
    "\n",
    "    Returns:\n",
    "    float: Updated resource state.\n",
    "\n",
    "    This function calculates the updated resource state based on the extraction level and environmental impact factor.\n",
    "    \"\"\"\n",
    "    return n + e1 * (z - n)\n",
    "\n",
    "## Heuristics and updating ##\n",
    "def predict_zl(z_evolution, heuristic, b1, b2, b3):\n",
    "    \"\"\"\n",
    "    Predict the next extraction level using different heuristics.\n",
    "\n",
    "    Parameters:\n",
    "    - z_evolution (numpy.ndarray): Array of past extraction levels.\n",
    "    - heuristic (int): Heuristic type (0-3).\n",
    "    - b1, b2, b3 (float): Heuristic parameters.\n",
    "\n",
    "    Returns:\n",
    "    float: Predicted extraction level.\n",
    "\n",
    "    This function predicts the next extraction level based on past observations and the chosen heuristic.\n",
    "    \"\"\"\n",
    "    t = len(z_evolution)\n",
    "    if heuristic == 0: # adaptive expectation on zl (essentially exponential weights on all memories)\n",
    "        expec = np.sum([z_evolution[i] * b1 * (1-b1) ** (t-i-1) for i in range(t)])\n",
    "    elif heuristic == 1: # trend-chasing expectation on zl\n",
    "        if t > 1:\n",
    "            expec = z_evolution[-1] + b2 * (z_evolution[-1] - z_evolution[-2]) \n",
    "        else: \n",
    "            expec = z_evolution[-1]  # no trend to extrapolate so just previous observation\n",
    "    elif heuristic == 2: # contrarian (trend-chasing) expectation on zl\n",
    "        if t > 1:\n",
    "            expec = z_evolution[-1] + b3 * (z_evolution[-1] - z_evolution[-2]) \n",
    "        else: \n",
    "            expec = z_evolution[-1]  # no trend to extrapolate so just previous observation\n",
    "    else: # anchoring and adjustment expectation on zl\n",
    "        if t > 1:\n",
    "            expec = 0.5 * (np.mean(z_evolution) + z_evolution[-1]) + (z_evolution[-1] - z_evolution[-2]) \n",
    "        else: \n",
    "            expec = z_evolution[-1] # no trend to extrapolate so just anchor point\n",
    "    return np.clip(expec, 0, 1)\n",
    "\n",
    "def update_heuristic(old_utilities, previous_z, previous_expectations, current_heuristic, h_upd_chance, eta, choice_intensity):\n",
    "    \"\"\"\n",
    "    Update the chosen heuristic for an agent based on utility and chance.\n",
    "\n",
    "    Parameters:\n",
    "    - old_utilities (numpy.ndarray): Array of utilities for different heuristics.\n",
    "    - previous_z (float): Previous extraction level.\n",
    "    - previous_expectations (numpy.ndarray): Array of past expectations.\n",
    "    - current_heuristic (int): Current heuristic choice.\n",
    "    - h_upd_chance (float): Probability of updating the heuristic.\n",
    "    - eta (float): Importance of past utilities for current heuristic utility.\n",
    "    - choice_intensity (float): Intensity of choice.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[int, numpy.ndarray]: Updated heuristic choice and new utilities.\n",
    "    \"\"\"\n",
    "    new_utilities = np.array([-(previous_z - previous_expectations[i]) ** 2 + eta * old_utilities[i] for i in range(len(previous_expectations))])\n",
    "    draw1 = np.random.uniform()\n",
    "    if draw1 < h_upd_chance: # only update h_upd_chance percent of the time\n",
    "        sum = np.sum(np.exp(choice_intensity * new_utilities))\n",
    "        draw2 = np.random.uniform()\n",
    "        count = np.exp(choice_intensity * new_utilities[0])\n",
    "        id = 0\n",
    "        while draw2 >= count/sum:\n",
    "            id += 1\n",
    "            count += np.exp(choice_intensity * new_utilities[id])\n",
    "        return id, new_utilities\n",
    "    else:\n",
    "        return current_heuristic, new_utilities\n",
    "    \n",
    "## Agent decision-making ##\n",
    "def base_sigmoid(pd, degree, sigma):\n",
    "    \"\"\"\n",
    "    Calculate the sigmoid function used in agent decision-making, used for the probability of extracting with high effort.\n",
    "\n",
    "    Parameters:\n",
    "    - pd (float): Payoff difference.\n",
    "    - degree (float): Agent's degree in the network.\n",
    "    - sigma (float): Sigmoid parameter.\n",
    "\n",
    "    Returns:\n",
    "    float: Sigmoid value.\n",
    "    \"\"\"\n",
    "    beta = np.sqrt(degree) / sigma \n",
    "    return 1 / (1 + np.exp(-beta * pd))\n",
    "\n",
    "def action(n, z_evolution, d0, a1l, a1h, a2l, a2h, a3l, a3h, P, b1, b2, b3, heuristic, degree, sigma):\n",
    "    \"\"\"\n",
    "    Make a decision regarding extraction based on heuristics and payoffs.\n",
    "\n",
    "    Parameters:\n",
    "    - n (float): Current resource state.\n",
    "    - z_evolution (numpy.ndarray): Array of past extraction levels.\n",
    "    - d0 (float): Constant in the payoff function.\n",
    "    - a1l, a1h, a2l, a2h, a3l, a3h (float): Parameters in the payoff function.\n",
    "    - P (int): Number of agents in the network.\n",
    "    - b1, b2, b3 (float): Heuristic parameters.\n",
    "    - heuristic (int): Chosen heuristic.\n",
    "    - degree (float): Agent's degree in the network.\n",
    "    - sigma (float): Sigmoid parameter.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[int, float, float]: Action (0 or 1), predicted extraction level, and payoff difference.\n",
    "    \"\"\"\n",
    "    ze_t = predict_zl(z_evolution, heuristic, b1, b2, b3)\n",
    "    pd = d0 + (a1h-a1l) * (ze_t * (P-1)/P) + (a2h - a2l) * n + (a3h - a3l) * n * ze_t * (P-1)/P - a1l/P - a3l * n / P \n",
    "\n",
    "    random_draw = np.random.uniform()\n",
    "    cutoff = base_sigmoid(pd, degree, sigma)\n",
    "    if random_draw < cutoff:\n",
    "        return 1, ze_t, pd\n",
    "    else:\n",
    "        return 0, ze_t, pd\n",
    "    \n",
    "## Network functions ##\n",
    "def gen_net(P, ne):\n",
    "    \"\"\"\n",
    "    Generate an Erdös-Rényi or Barabasi-Albert network.\n",
    "\n",
    "    Parameters:\n",
    "    - P (int): Number of agents in the network.\n",
    "    - ne: link density in ER network (float) or number of new links per new node in BA network (int)\n",
    "\n",
    "    Returns:\n",
    "    Tuple[numpy.ndarray, numpy.ndarray]: Adjacency matrix and degree sequence.\n",
    "    \"\"\"\n",
    "    if type(ne) == np.int64 or type(ne) == int:\n",
    "        network = nx.barabasi_albert_graph(P,int(ne))\n",
    "    else:\n",
    "        network = nx.gnp_random_graph(P, ne)\n",
    "    A = nx.to_numpy_array(network) + np.eye(P)\n",
    "    return A, np.sum(A, axis=0) \n",
    "\n",
    "def construct_perceived_weights(A, impact):\n",
    "    \"\"\"\n",
    "    Construct perceived weights for environmental impact.\n",
    "\n",
    "    Parameters:\n",
    "    - A (numpy.ndarray): Adjacency matrix.\n",
    "    - impact (float): inequality of environmental impact.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Perceived weights for environmental impact.\n",
    "    \"\"\"\n",
    "    P = A.shape[0]\n",
    "    SA = np.sum(A, axis=0)\n",
    "    SA_power = SA ** impact\n",
    "    SA_imp_matrix = np.tile(SA_power, (P, 1))\n",
    "    \n",
    "    if impact == 0:\n",
    "        perceived_weights = A / np.tile(np.sum(A, axis=1), (P, 1)).T \n",
    "    else:\n",
    "        perceived_weights = A * SA_imp_matrix / (np.sum((SA * A) ** impact, axis=1)[:, np.newaxis])\n",
    "    \n",
    "    return perceived_weights\n",
    "\n",
    "## Other functions ##\n",
    "def mean_conf(results, x):\n",
    "    \"\"\"\n",
    "    Calculate mean and confidence interval for simulation results.\n",
    "\n",
    "    Parameters:\n",
    "    - results (numpy.ndarray): Simulation results.\n",
    "    - x (int): Number of simulations.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[numpy.ndarray, numpy.ndarray]: Mean and confidence interval.\n",
    "    \"\"\"\n",
    "    mean = np.mean(results, axis=0)\n",
    "    conf = 1.96 * np.std(results, axis=0) / np.sqrt(x)\n",
    "    return mean, conf \n",
    "\n",
    "def br_longrun(d0, a1l, a1h, a2l, a2h, a3l, a3h, P):\n",
    "    \"\"\"\n",
    "    Calculate the full-information equilibrium extraction level.\n",
    "\n",
    "    Parameters:\n",
    "    - d0, a1l, a1h, a2l, a2h, a3l, a3h (float): Parameters in the payoff function.\n",
    "    - P (float): Number of agents.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Long-run equilibrium extraction level.\n",
    "\n",
    "    This function calculates the long-run equilibrium extraction level based on the payoff function parameters.\n",
    "    \"\"\"\n",
    "    if a3l == a3h:\n",
    "        num = a1l / P - d0\n",
    "        den = (a1h - a1l) * (P - 1) / P + a2h - a2l - a3l/P\n",
    "        zlr = np.array([num/den, num/den])\n",
    "    else:\n",
    "        a = d0-a1l/P\n",
    "        b = (a1h-a1l) * (P-1) + a2h-a2l - a3l/P\n",
    "        c = (a3h-a3l) * (P-1)/P\n",
    "        D = b ** 2 - 4 * a * c\n",
    "        zlr = np.array([(-b + np.sqrt(D)) / (2*a), (-b - np.sqrt(D)) / (2*a)])\n",
    "    np.put(zlr, np.append(np.where(zlr>1), np.where(zlr<0)),np.nan)\n",
    "    return zlr\n",
    "\n",
    "def calc_moment(degrees, P, n_bins=20, moment=3):\n",
    "    \"\"\"\n",
    "    Calculate a moment of the degree distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - degrees (numpy.ndarray): Degree sequence.\n",
    "    - P (int): Number of agents in the network.\n",
    "    - n_bins (int): Number of bins for histogram.\n",
    "    - moment (int): Order of the moment to calculate.\n",
    "\n",
    "    Returns:\n",
    "    float: Calculated moment.\n",
    "    \"\"\"\n",
    "    weights, bin_edges = np.histogram(degrees, bins=n_bins, range=(0,P))\n",
    "    values = np.array([np.mean([bin_edges[i], bin_edges[i+1]]) for i in range(n_bins)])\n",
    "    ave = np.average(values, weights=weights)\n",
    "    var = np.sum(np.array([weights[i]*(values[i]-ave)**moment for i in range(n_bins)]))\n",
    "    return var ** (1/moment) / P\n",
    "\n",
    "## Model ##\n",
    "def model(n, z, P, ne, T, e1, d0, a1l, a1h, a2l, a2h, a3l, a3h, impact, sigma, b1, b2, b3, h_upd_chance, eta, choice_intensity):\n",
    "    \"\"\"\n",
    "    Simulate the agent-based model.\n",
    "\n",
    "    Parameters:\n",
    "    - n (float): Initial resource state.\n",
    "    - z (float): Initial extraction level.\n",
    "    - P (int): Number of agents in the network.\n",
    "    - ne: link density in ER network (float) or number of new links per new node in BA network (int).\n",
    "    - T (int): Number of time steps.\n",
    "    - e1 (float): Relative speed of environmental dynamics to strategic dynamics.\n",
    "    - d0, a1l, a1h, a2l, a2h, a3l, a3h (float): Parameters in the payoff function.\n",
    "    - impact (float): Inequality of environmental impact.\n",
    "    - sigma (float): Sigmoid parameter.\n",
    "    - b1, b2, b3 (float): Heuristic parameters.\n",
    "    - h_upd_chance (float): Probability of updating the heuristic.\n",
    "    - eta (float): Importance of past utilities for current heuristic utility.\n",
    "    - choice_intensity (float): Intensity of choice.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, float]:\n",
    "    - n_evolution (numpy.ndarray): Evolution of resource state.\n",
    "    - z_evolution (numpy.ndarray): Evolution of extraction level.\n",
    "    - z_predictions (numpy.ndarray): Predicted extraction levels.\n",
    "    - pd (numpy.ndarray): Payoff differences.\n",
    "    - heuristic_fraction (numpy.ndarray): Fraction of agents using each heuristic.\n",
    "    - observed_z (numpy.ndarray): Observed extraction levels.\n",
    "    - acting_p (numpy.ndarray): Index of acting agent.\n",
    "    - degrees (numpy.ndarray): Degree sequence of the network.\n",
    "    - skew (float): Skewness of the degree distribution.\n",
    "    \"\"\"\n",
    "    z = 1 # with environmental weights, initial conditions for z are a bit tricky \n",
    "    A, degrees = gen_net(P, ne) # form the network\n",
    "    skew = calc_moment(degrees, P)\n",
    "    n_evolution = np.ones((T*P+1)) * n \n",
    "    z_evolution = np.ones((T*P+1)) * z       \n",
    "    actions = np.random.permutation(np.append(np.zeros(int(z*P)), np.ones(P-int(z*P)))) \n",
    "    z_predictions = np.empty(T*P)\n",
    "    observed_z = np.empty(T*P)\n",
    "    pd = np.empty(T*P)\n",
    "    heuristic = np.random.randint(low=0, high=4, size=P) # period 1 and 2 everyone uses a random heuristic; high 4 because 4 heuristics\n",
    "    heuristic_fraction = np.zeros((T*P+1, 4))\n",
    "    heuristic_fraction[0,:] = np.array([np.count_nonzero(heuristic == h)/P for h in range(4)])\n",
    "    utilities = np.zeros((P, 4)) # four heuristics \n",
    "    memory = np.zeros((P,P)) * np.nan \n",
    "    indices = np.where(A == 1)\n",
    "    memory[indices] = actions[indices[1]]\n",
    "    perceived_z = {i: np.array([]) for i in range(P)}\n",
    "    perceived_n = np.ones(P) * n\n",
    "    acting_p = np.empty(T*P)\n",
    "    weights = degrees ** impact / np.sum(degrees ** impact) # these are used to update the resource state\n",
    "    perceived_z_weights = construct_perceived_weights(A - np.eye(P), impact) # these have no self link and are used for taking into account environmental impact for forming expectations\n",
    "    perceived_n_weights = construct_perceived_weights(A, impact) # these have a self link and are used for taking into account environmental impact for resource state perception updates\n",
    "\n",
    "    for i in range(T*P):\n",
    "        # choose a random player that acts\n",
    "        id = np.random.randint(0,P)\n",
    "        acting_p[i] = id\n",
    "        old_heuristic = heuristic[id]\n",
    "        old_action = actions[id]\n",
    "        observation = memory[id].copy()\n",
    "        observation[id] = np.nan # since want to predict the other users' behaviour, not your own\n",
    "        perceived_z[id] = np.append(perceived_z[id], 1-np.nansum(observation * perceived_z_weights[id,:])) # take a snapshot of behaviour each time you act\n",
    "        observed_z[i] = perceived_z[id][-1]\n",
    "\n",
    "        # choose heuristic\n",
    "        if len(perceived_z[id]) > 1: # can't update heuristics in first two periods as requires two previous expectations\n",
    "            first_order_expectations = np.array([predict_zl(perceived_z[id][:-1], h, b1, b2, b3) for h in range(4)])\n",
    "            heuristic[id], utilities[id,:] = update_heuristic(utilities[id,:], perceived_z[id][-1], first_order_expectations, heuristic[id], \n",
    "                                                    h_upd_chance, eta, choice_intensity)\n",
    "\n",
    "        # choose which action maximizes discounted expected payoff\n",
    "        actions[id], z_predictions[i], pd[i] = action(perceived_n[id], perceived_z[id], d0, a1l, a1h, a2l, a2h, a3l, a3h, P, b1, b2, b3, heuristic[id], degrees[id], sigma)\n",
    "        \n",
    "        # update agents' memories and perceived_n\n",
    "        if A[:, id].any():\n",
    "            memory[:, id] = np.where(A[:, id], actions[id], memory[:, id])\n",
    "        perceived_n = update_n(perceived_n, 1-np.nansum(memory * perceived_n_weights, axis=1), e1/P) # each time someone acts, update perception of n using your view of what everyone is doing\n",
    "\n",
    "        # update z, environment, and heuristic fraction\n",
    "        diff = actions[id] - old_action\n",
    "        if diff == 1:\n",
    "            z_evolution[i+1] = z_evolution[i] - weights[id] \n",
    "        elif diff == -1:\n",
    "            z_evolution[i+1] = z_evolution[i] + weights[id] \n",
    "        else:\n",
    "            z_evolution[i+1] = z_evolution[i]\n",
    "        n_evolution[i+1] = update_n(n_evolution[i], z_evolution[i+1], e1/P)\n",
    "\n",
    "        heuristic_fraction[i+1,:] = heuristic_fraction[i,:]\n",
    "        heuristic_fraction[i+1, old_heuristic] -= 1/P \n",
    "        heuristic_fraction[i+1, heuristic[id]] += 1/P \n",
    "        \n",
    "    return n_evolution, z_evolution, z_predictions, pd, heuristic_fraction, observed_z, acting_p, degrees, skew\n",
    "\n",
    "## Run model many times ##\n",
    "def run_model(x, n, z, P, ne, T, e1, d0, a1l, a1h, a2l, a2h, a3l, a3h, impact, sigma, b1, b2, b3, h_upd_chance, eta, choice_intensity):\n",
    "    \"\"\"\n",
    "    Run the agent-based model multiple times and aggregates the results for analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - x (int): Number of simulations.\n",
    "    - n (float): Initial resource state.\n",
    "    - z (float): Initial extraction level.\n",
    "    - P (int): Number of agents in the network.\n",
    "    - ne: link density in ER network (float) or number of new links per new node in BA network (int).\n",
    "    - T (int): Number of time steps.\n",
    "    - e1 (float): Relative speed of environmental dynamics to strategic dynamics.\n",
    "    - d0, a1l, a1h, a2l, a2h, a3l, a3h (float): Parameters in the payoff function.\n",
    "    - impact (float): Inequality of environmental impact.\n",
    "    - sigma (float): Sigmoid parameter.\n",
    "    - b1, b2, b3 (float): Heuristic parameters.\n",
    "    - h_upd_chance (float): Probability of updating the heuristic.\n",
    "    - eta (float): Importance of past utilities for current heuristic utility.\n",
    "    - choice_intensity (float): Intensity of choice.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray]:\n",
    "    - n_results (numpy.ndarray): Evolution of resource state for each simulation.\n",
    "    - pd_results (numpy.ndarray): Payoff differences for each simulation.\n",
    "    - acting_p_results (numpy.ndarray): Index of acting agent for each simulation.\n",
    "    - degrees_results (numpy.ndarray): Degree sequences for each simulation.\n",
    "    - heuristic_results (numpy.ndarray): Fraction of agents using each heuristic for each simulation.\n",
    "    \"\"\"\n",
    "    n_results = np.zeros((x, T*P+1))\n",
    "    pd_results = np.zeros((x, T*P))\n",
    "    acting_p_results = np.zeros((x, T*P))\n",
    "    degrees_results = np.zeros((x, P))\n",
    "    heuristic_results = np.zeros((x, T*P+1, 4))\n",
    "\n",
    "    for i in tqdm(range(x)):\n",
    "        n_evolution, z_evolution, zl_predictions, pd, heuristic_fraction, observed_z, acting_p, degrees, skew \\\n",
    "            = model(n, z, P, ne, T, e1, d0, a1l, a1h, a2l, a2h, a3l, a3h, impact, sigma, b1, b2, b3, h_upd_chance, eta, choice_intensity)\n",
    "        n_results[i,:] = n_evolution \n",
    "        pd_results[i,:] = pd \n",
    "        acting_p_results[i,:] = acting_p  \n",
    "        degrees_results[i,:] = degrees\n",
    "        heuristic_results[i,:,:] = heuristic_fraction\n",
    "    \n",
    "    return n_results, pd_results, acting_p_results, degrees_results, heuristic_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
